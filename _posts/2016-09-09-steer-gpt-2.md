---
title: 'Steer GPT-2'
date: 2019-12-06T02:10:00+01:00
draft: false
---

[](https://github.com/uber-research/PPLM#pplm)PPLM
==================================================

This repository contains the original code used to run the Plug and Play Language Model (PPLM).

It has also been integrated into the **[ðŸ¤—/Transformers](https://github.com/huggingface/transformers/tree/master/examples/pplm)** repository.

[![header image](https://github.com/uber-research/PPLM/raw/master/imgs/headfigure.png)](https://github.com/uber-research/PPLM/blob/master/imgs/headfigure.png)

[](https://github.com/uber-research/PPLM#plug-and-play-language-models-a-simple-approach-to-controlled-text-generation)Plug and Play Language Models: a Simple Approach to Controlled Text Generation
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Authors: [Sumanth Dathathri](https://dathath.github.io/), [Andrea Madotto](https://andreamad8.github.io/), Janice Lan, Jane Hung, Eric Frank, [Piero Molino](https://w4nderlu.st/), [Jason Yosinski](http://yosinski.com/), and [Rosanne Liu](http://www.rosanneliu.com/)

PPLM allows a user to flexibly plug in one or more tiny attribute models representing the desired steering objective into a large, unconditional language model (LM). The method has the key property that it uses the LM _as is_â€”no training or fine-tuning is requiredâ€”which enables researchers to leverage best-in-class LMs even if they do not have the extensive hardware required to train them.

Paper link: [https://arxiv.org/abs/1912.02164](https://arxiv.org/abs/1912.02164)

Blog link: [https://eng.uber.com/pplm](https://eng.uber.com/pplm)

Colab link to test it out without any setup: [https://colab.research.google.com/drive/1Ux0Z4-ruiVtJ6jUk98uk6FqfvGHCOYL3](https://colab.research.google.com/drive/1Ux0Z4-ruiVtJ6jUk98uk6FqfvGHCOYL3)

[](https://github.com/uber-research/PPLM#setup)Setup
----------------------------------------------------

```
pip install -r requirements.txt
```

[](https://github.com/uber-research/PPLM#citation)Citation
----------------------------------------------------------

```
@article{dathathri2019plug, title={Plug and Play Language Models: a Simple Approach to Controlled Text Generation}, author={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu}, journal={arXiv preprint arXiv:1912.02164}, year={2019}, } 
```

[](https://github.com/uber-research/PPLM#pplm-bow)PPLM-BoW
----------------------------------------------------------

### [](https://github.com/uber-research/PPLM#example-command-for-bag-of-words-control)Example command for bag-of-words control

```
python run\_pplm.py -B military --cond\_text "The potato" --length 50 --gamma 1.5 --num\_iterations 3 --num\_samples 10 --stepsize 0.03 --window\_length 5 --kl\_scale 0.01 --gm\_scale 0.99 --colorama --sample
```

### [](https://github.com/uber-research/PPLM#tuning-hyperparameters-for-bag-of-words-control)Tuning hyperparameters for bag-of-words control

1.  Increase `--stepsize` to intensify topic control, and decrease its value to soften the control. `--stepsize 0` recovers the original uncontrolled GPT-2 model.
    
2.  If the language being generated is repetitive (For e.g. "science science experiment experiment"), there are several options to consider:  
    a) Reduce the `--stepsize`  
    b) Increase `--kl_scale` (the KL-loss coefficient) or decrease `--gm_scale` (the gm-scaling term)  
    c) Add `--grad-length xx` where xx is an (integer <= length, e.g. `--grad-length 30`).  
    

[](https://github.com/uber-research/PPLM#pplm-discrim)PPLM-Discrim
------------------------------------------------------------------

### [](https://github.com/uber-research/PPLM#example-command-for-discriminator-based-sentiment-control)Example command for discriminator based sentiment control

```
python run\_pplm.py -D sentiment --class\_label 2 --cond\_text "My dog died" --length 50 --gamma 1.0 --num\_iterations 10 --num\_samples 10 --stepsize 0.04 --kl\_scale 0.01 --gm\_scale 0.95 --sample
```

### [](https://github.com/uber-research/PPLM#tuning-hyperparameters-for-discriminator-control)Tuning hyperparameters for discriminator control

1.  Increase `--stepsize` to intensify topic control, and decrease its value to soften the control. `--stepsize 0` recovers the original uncontrolled GPT-2 model.
    
2.  Use `--class_label 3` for negative, and `--class_label 2` for positive
    

  
  
from Hacker News https://github.com/uber-research/PPLM